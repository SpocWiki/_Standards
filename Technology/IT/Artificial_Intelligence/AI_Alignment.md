Since AI is a statistical Model, its behavior is largely opaque 
and the random component makes it non-reliable. 

Over and over, Designers are surprised by the complex behavior of Neural Networks. 

Alignment tries to guard against the worst surprises. 

## Alignment Strategies 

### Reinforcement Learning with Human Feedback 

The Output is valued by Humans which give positive or negative 
human Feedback to strengthen or weaken this type of Output. 
This Work is very tedious and repetitive and typically done in low-income countries. 
It can be detrimental to mental health both through the content and the load. 

### Constitutional AI 
Rules similar to Asimov Laws 
This Alignment is done with 

### Explainable AI 

similar to [[#Reinforcement Learning with Human Feedback]] but not with Instances, 
but at a higher Level with Rules or Decision Trees. 

Unfortunately, generative AI manufactures the reasons a posteriori, 
instead of following a process. 

### Adversarial AI Training 

2 AIs try to train each other 

## Alignment Problems 

Generative AIs can be tricked to work around their Alignment in 97% of all cases. 
An efficient approach is to use one AI to hack the other AI. 
The Attacker changes Strategies (Confusion, Extortion, Flattery, ...) until it succeeds. 

## Alignment Regression 

Generative AIs react to Language. The Context can be changed until 
The AI can detect when it is being tested and changes its behavior accordingly. 
Similar to the Exhaust Faking in Automobiles. 





