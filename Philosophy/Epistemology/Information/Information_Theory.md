---
aliases:
  - Information Theory
has_id_wikidata: Q131222
creator: "[[_Standards/WikiData/WD~Claude_Shannon,92760]]"
part_of:
  - "[[_Standards/WikiData/WD~cognitive_science,147638]]"
  - "[[_Standards/WikiData/WD~mathematics,395]]"
  - "[[_Standards/WikiData/WD~computer_science,21198]]"
instance_of:
  - "[[_Standards/WikiData/WD~branch_of_science,2465832]]"
  - "[[_Standards/WikiData/WD~academic_discipline,11862829]]"
maintained_by_WikiProject: "[[_Standards/WikiData/WD~WikiProject_Mathematics,8487137]]"
subclass_of: "[[_Standards/WikiData/WD~mathematical_theory,20026918]]"
facet_of: "[[_Standards/WikiData/WD~statistics,12483]]"
different_from: "[[_Standards/WikiData/WD~information_science,16387]]"
ACM_Classification_Code_2012_: "10003712"
Library_of_Congress_Classification: Q350-Q390
image: http://commons.wikimedia.org/wiki/Special:FilePath/Binary%20erasure%20channel.svg
Stack_Exchange_tag:
  - https://math.stackexchange.com/tags/information-theory
  - https://quantumcomputing.stackexchange.com/tags/information-theory
IEV_number: 171-07-01
UMLS_CUI: C0021429
Commons_category: Information theory
GitHub_topic: information-theory
PhilPapers_topic: information-theory
MeSH_tree_code: L01.488
Krugosvet_article_archived_: nauka_i_tehnika/matematika/INFORMATSII_TEORIYA.html
---

# [[Information_Theory]] 

#is_/same_as :: [[WikiData/WD~Information_theory,131222|WD~Information_theory,131222]] 

## #has_/text_of_/abstract 

> **Information Theory** is the mathematical study of the 
> quantification, storage, and communication of information. 
> 
> The field was established and formalized by Claude Shannon in the 1940s, 
> though early contributions were made in the 1920s 
> through the works of Harry Nyquist and Ralph Hartley. 
> It is at the intersection of electronic engineering, mathematics, statistics, computer science, neurobiology, physics, and electrical engineering.
>
> A key measure in information theory is entropy. 
> Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (which has two equally likely outcomes) provides less information (lower entropy, less uncertainty) than identifying the outcome from a roll of a die (which has six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.
>
> Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet and artificial intelligence. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, signal processing, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, molecular dynamics, black holes, quantum computing, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection, the analysis of music, art creation, imaging system design, study of outer space, the dimensionality of space, and epistemology.
>
> [Wikipedia](https://en.wikipedia.org/wiki/Information%20theory) 

